{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../Tools\")\n",
    "import numpy as np\n",
    "import ProcDoc\n",
    "from Evaluate import EvaluateModel\n",
    "import math\n",
    "from math import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}                # content of document (doc, content)\n",
    "background_model = {}    # word count of 2265 document (word, number of words)\n",
    "general_model = {}\n",
    "query = {}                # query\n",
    "\n",
    "query_lambda = 0.9\n",
    "doc_lambda = 0.3\n",
    "min_factor = 0.0000000001\n",
    "\n",
    "document_path = \"../Corpus/TDT2/SPLIT_DOC_WDID_NEW\"\n",
    "query_path = \"../Corpus/TDT2/QUERY_WDID_NEW\"\n",
    "relevance_path = \"../Corpus/TDT2/AssessmentTrainSet/AssessmentTrainSet.txt\"\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_background_dict_n(w_dicts, q_dicts, vocab_size=51253):\n",
    "    BGTraingSetDict = {}\n",
    "    BGTraingSetDict_prob = {}\n",
    "    super_dict = {}\n",
    "    #add docs to bg model \n",
    "    for top, d in w_dicts.iteritems():\n",
    "        for (k, v) in d.iteritems():\n",
    "            super_dict[k] = v if k not in super_dict else super_dict[k] + v\n",
    "    #add query to bg model\n",
    "    for top, d in q_dicts.iteritems():\n",
    "        for (k, v) in d.iteritems():\n",
    "            super_dict[k] = v if k not in super_dict else super_dict[k] + v\n",
    "    \n",
    "    for k, c in super_dict.iteritems():\n",
    "        prob = float(float(c)/vocab_size)\n",
    "        exp_prob = math.exp(prob)\n",
    "        #print(k, c, prob)\n",
    "        BGTraingSetDict[k] = exp_prob\n",
    "        BGTraingSetDict_prob[k] = prob             \n",
    "    return BGTraingSetDict, BGTraingSetDict_prob\n",
    "\n",
    "def read_background_np_n(bg_dict):\n",
    "    vocab_size = 51253\n",
    "    obj_vec = np.zeros(vocab_size)\n",
    "    for k, v in bg_dict.iteritems():\n",
    "        obj_vec[int(k)] = v\n",
    "    #print(obj_vec[:1000])\n",
    "    return obj_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2np_n(ori_dict, IDs_list = None, vocab_size=51253):\n",
    "    num_tar = len(list(ori_dict.keys()))\n",
    "    obj_vec = np.zeros((num_tar, vocab_size))\n",
    "    if IDs_list is None:\n",
    "        IDs_list = list(ori_dict.keys())\n",
    "    for idx, o_id in enumerate(IDs_list):\n",
    "        for o_wid, o_wc in ori_dict[o_id].items():\n",
    "            #if o_wc:\n",
    "                #print(o_wid, o_wc)\n",
    "            obj_vec[idx][int(o_wid)] = o_wc\n",
    "    return obj_vec, IDs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(docs_words_dict):\n",
    "    doc_freq = {}\n",
    "    total_docs = len(docs_words_dict.keys()) * 1.0\n",
    "    # compute document frequency\n",
    "    for doc, words_dict in docs_words_dict.items():\n",
    "        for word in words_dict.keys():\n",
    "            if word in doc_freq:\n",
    "                doc_freq[word] += 1\n",
    "            else:\n",
    "                doc_freq[word] = 1\n",
    "    # tfidf\n",
    "    set_tfidf = {}\n",
    "    for doc, words_list in docs_words_dict.items():\n",
    "        t_doc_tfidf = {}\n",
    "        for word, tf in words_list.items():\n",
    "            idf = 1 / log(1 + total_docs / doc_freq[word])\n",
    "            t_doc_tfidf[word] = (1 + log(tf)) * idf\n",
    "        set_tfidf[doc] = t_doc_tfidf\n",
    "    return set_tfidf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_query(qnparr, d):\n",
    "    dnparr = np.array(docs_words_dict[d].keys())\n",
    "    intercept = np.intersect1d(dnparr, qnparr)    \n",
    "    return len(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare query top 10 doc for feedback module\n",
    "def score_doc_by_query(query_wordcount_sp, q):\n",
    "    score_doc_dict = {}\n",
    "    qnparr = np.array(query_wordcount_sp.keys())\n",
    "    \n",
    "    for doc, count in doc_wordcount.items():\n",
    "        dnparr = np.array(doc_wordcount[doc].keys())\n",
    "        s = len(np.intersect1d(dnparr, qnparr))\n",
    "        score_doc_dict.update({doc:s})\n",
    "    \n",
    "    sorted_dict = sorted(score_doc_dict.items(), key=lambda kv: kv[1])\n",
    "    top_10 = sorted_dict[::-1][:10]\n",
    "    doc_mdl_to_idx = {}\n",
    "    for k, v in top_10:\n",
    "        arr_idx = np.where(doc_IDs_t_np==k)\n",
    "        doc_mdl_to_idx.update({k:arr_idx})\n",
    "    return doc_mdl_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feedback doc model teta_di\n",
    "def query_feedback_mdl(doc_mdl_to_idx, l_lambda):\n",
    "    doc_mdl_to_log = {}\n",
    "    sum_log_di = np.zeros(len(doc_mdl[0]))\n",
    "    for doc, mdl_idx in  doc_mdl_to_idx.iteritems():\n",
    "        #print(doc)\n",
    "        np_ml = np.array(doc_mdl[mdl_idx])\n",
    "        np_ml[np_ml < min_factor] = min_factor\n",
    "        dl_log = np.log(np_ml)\n",
    "        sum_log_di = np.add(dl_log, sum_log_di)\n",
    "        doc_mdl_to_log.update({doc : dl_log})    \n",
    "\n",
    "    sum_log_di_norm = sum_log_di/len(doc_mdl_to_idx)    \n",
    "    u_lambda = l_lambda/(1-l_lambda)\n",
    "    fb_mdl = np.zeros(len(doc_mdl[0]))\n",
    "    fb_mdl = np.add(u_lambda*sum_log_di_norm, u_lambda*bg_mdl_log)\n",
    "    return np.exp(fb_mdl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smooth \n",
    "def proccess_query_mdl(query_idx):\n",
    "    np_query_mdl = np.array(query_mdl[query_idx])\n",
    "    np_query_mdl[np_query_mdl < min_factor] = min_factor\n",
    "    return np_query_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccess_bg_mdl():\n",
    "    background_model_np_prob[background_model_np_prob < min_factor] = min_factor\n",
    "    return np.log(background_model_np_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count, takewhile\n",
    "def frange(start, stop, step):\n",
    "    return takewhile(lambda x: x< stop, count(start, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document model, calc log(p(x))\n",
    "data = ProcDoc.read_file(document_path)\n",
    "doc_wordcount = ProcDoc.doc_preprocess(data)\n",
    "doc_tfidf = TFIDF(doc_wordcount) \n",
    "doc_unigram = ProcDoc.unigram(dict(doc_tfidf))\n",
    "doc_mdl, doc_IDs = ProcDoc.dict2np(doc_unigram)\n",
    "\n",
    "# query model\n",
    "query = ProcDoc.read_file(query_path)\n",
    "query = ProcDoc.query_preprocess(query)\n",
    "query_wordcount = {}\n",
    "\n",
    "for q, q_content in query.items():\n",
    "    query_wordcount[q] = ProcDoc.word_count(q_content, {})\n",
    "\n",
    "query_tfidf = TFIDF(query_wordcount) \n",
    "query_unigram = ProcDoc.unigram(dict(query_tfidf))\n",
    "#query_model = query_unigram\n",
    "query_mdl, query_IDs = ProcDoc.dict2np(query_unigram)\n",
    "\n",
    "# background_model, calc log(p(x)), p(x)=word_count/vocab_count \n",
    "background_model_exp, background_model_prob = read_background_dict_n(doc_wordcount, query_wordcount)\n",
    "background_model_np_exp = read_background_np_n(background_model_exp)\n",
    "bg_mdl_log = proccess_bg_mdl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document smoothing \n",
    "for doc_idx in xrange(doc_mdl.shape[0]):\n",
    "    doc_vec = doc_mdl[doc_idx]\n",
    "    doc_mdl[doc_idx] = doc_lambda * doc_vec + (1 - doc_lambda) * background_model_np_prob\n",
    "    \n",
    "# query smoothing \n",
    "for query_idx in xrange(query_mdl.shape[0]):\n",
    "    query_vec = query_mdl[query_idx]\n",
    "    query_mdl[query_idx] =  query_lambda * query_vec + (1 - query_lambda) * background_model_np_prob    \n",
    "    \n",
    "np_query_ids = np.array(query_IDs)    \n",
    "doc_mdl_t, doc_IDs_t = dict2np_n(doc_unigram)\n",
    "doc_IDs_t_np = np.array(doc_IDs_t)\n",
    "\n",
    "# prepare the doc mdl\n",
    "for doc_idx in range(np.shape(doc_mdl)[0]):    \n",
    "    doc_mdl_vec = doc_mdl[doc_idx]\n",
    "    doc_mdl_vec[doc_mdl_vec < min_factor] = min_factor    \n",
    "    doc_mdl[doc_idx] = doc_mdl_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: mean Average Precision\n",
      "0.0154112633394\n",
      "Eval: mean Average Precision\n",
      "0.016503497237\n",
      "Eval: mean Average Precision\n",
      "0.0183559552589\n",
      "Eval: mean Average Precision\n",
      "0.0307635333016\n",
      "Eval: mean Average Precision\n",
      "0.350608215229\n",
      "Eval: mean Average Precision\n",
      "0.405355498781\n",
      "Eval: mean Average Precision\n",
      "0.444158241578\n",
      "Eval: mean Average Precision\n",
      "0.480085621133\n",
      "Eval: mean Average Precision\n",
      "0.460551245323\n",
      "Eval: mean Average Precision\n",
      "0.461046295202\n",
      "Eval: mean Average Precision\n",
      "0.461740739646\n"
     ]
    }
   ],
   "source": [
    "query_mdl_final, query_final_IDs = ProcDoc.dict2np(query_unigram)\n",
    "\n",
    "#l_lambda = 0.8\n",
    "alpha = 0.65\n",
    "\n",
    "for l_lambda in  frange(0, 1, 0.1):\n",
    "    for q, idx in query_wordcount.items():   \n",
    "        query_idx = np.where(np_query_ids==q)\n",
    "        doc_mdl_to_idx = score_doc_by_query(query_wordcount[q], q)\n",
    "        #print(s)\n",
    "        fb_mdl_exp = query_feedback_mdl(doc_mdl_to_idx, l_lambda)    \n",
    "        np_query_mdl = proccess_query_mdl(query_idx)\n",
    "        #print(np_query_mdl)\n",
    "        query_mdl_final[query_idx] = \\\n",
    "            np.add((1-alpha)*np_query_mdl, alpha*fb_mdl_exp)  \n",
    "    \n",
    "    #multi fb query mdl and doc mdl - get sorted doc relevane \n",
    "    results = np.argsort(-np.dot(query_mdl_final, np.log(doc_mdl.T)), axis = 1)\n",
    "    #arrange doc ranking by query\n",
    "    qry_docs_ranking = {}\n",
    "    for q_idx, q_ID in enumerate(query_IDs):\n",
    "        docs_ranking = []\n",
    "        for doc_idx in results[q_idx]:\n",
    "            docs_ranking.append(doc_IDs[doc_idx])\n",
    "        qry_docs_ranking[q_ID] = docs_ranking\n",
    "        \n",
    "    #evaluate score by compare to relevance     \n",
    "    eval_mdl = EvaluateModel(relevance_path)\n",
    "    rel_set = eval_mdl.getAset()\n",
    "    mAP = eval_mdl.mAP(qry_docs_ranking)\n",
    "    print(mAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
